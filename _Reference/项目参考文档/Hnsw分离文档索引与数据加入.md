## SurrealDB å‘é‡æ’å…¥ä¸HNSWå¼‚æ­¥æ„å»ºåˆ†ç¦»æœºåˆ¶
è¿™ä¸ª codemap è¯¦ç»†å±•ç¤ºäº† SurrealDB å¦‚ä½•é€šè¿‡ CONCURRENTLY å…³é”®å­—å’Œ IndexBuilder ç»„ä»¶å®ç°å‘é‡æ•°æ®æ’å…¥ä¸ HNSW ç´¢å¼•æ„å»ºçš„åˆ†ç¦»ã€‚å…³é”®æµç¨‹åŒ…æ‹¬ï¼šå¼‚æ­¥ç´¢å¼•æ„å»ºå¯åŠ¨ [1c]ã€IndexBuilder ä»»åŠ¡ç®¡ç† [2c]ã€æ–‡æ¡£æ¶ˆè´¹æœºåˆ¶ [3a]ã€å¢é‡é˜Ÿåˆ—ç®¡ç† [4c]ã€ä¸¤é˜¶æ®µæ„å»ºæµç¨‹ [5b]ï¼Œä»¥åŠ HNSW å‘é‡æ’å…¥ [6b]ã€‚è¿™ç§è®¾è®¡å®ç°äº†å¿«é€Ÿæ•°æ®å†™å…¥å’Œåå°æ…¢é€Ÿç´¢å¼•æ„å»ºçš„è§£è€¦ã€‚
### 1. ä½¿ç”¨ CONCURRENTLY å…³é”®å­—å¯åŠ¨å¼‚æ­¥ç´¢å¼•æ„å»º
å±•ç¤ºå¦‚ä½•é€šè¿‡ SQL è¯­æ³•è§¦å‘å¼‚æ­¥ç´¢å¼•æ„å»ºï¼Œå®ç°æ•°æ®æ’å…¥ä¸ç´¢å¼•æ„å»ºçš„åˆ†ç¦»
### 1a. CONCURRENTLY å…³é”®å­—å¤„ç† (`index.rs:166`)
åœ¨ SQL è§£æä¸­è¯†åˆ« CONCURRENTLY å…³é”®å­—
```text
if self.concurrently {
			write!(f, " CONCURRENTLY")?
		}
```
### 1b. å¯åŠ¨ç´¢å¼•æ„å»ºæµç¨‹ (`index.rs:138`)
æ ¹æ® concurrently å‚æ•°å†³å®šæ˜¯å¦å¼‚æ­¥æ„å»ºç´¢å¼•
```text
run_indexing(stk, ctx, opt, doc, &index_def, !self.concurrently).await?;
```
### 1c. è°ƒç”¨ IndexBuilder (`index.rs:210`)
å°†ç´¢å¼•æ„å»ºä»»åŠ¡äº¤ç»™ IndexBuilder å¤„ç†
```text
let rcv = ctx
			.get_index_builder()
			.ok_or_else(|| Error::unreachable("No Index Builder"))?
			.build(ctx, opt.clone(), ns, db, index.clone().into(), _blocking)?;
```
### 2. IndexBuilder å¼‚æ­¥æ„å»ºæœºåˆ¶
å±•ç¤º IndexBuilder å¦‚ä½•ç®¡ç†åå°ç´¢å¼•æ„å»ºä»»åŠ¡ï¼Œå¤„ç†å¢é‡æ•°æ®
### 2a. æ„å»ºæ–¹æ³•å…¥å£ (`index.rs:186`)
IndexBuilder çš„ä¸»è¦æ„å»ºæ¥å£
```text
pub(crate) fn build(
		&self,
		ctx: &Context,
		opt: Options,
		ns: NamespaceId,
		db: DatabaseId,
		ix: Arc<IndexDefinition>,
		blocking: bool,
	) -> Result<Option<Receiver<Result<()>>>> {
```
### 2b. å¯åŠ¨æ„å»ºä»»åŠ¡ (`index.rs:211`)
åˆ›å»ºåå°æ„å»ºä»»åŠ¡
```text
let ib = self.start_building(ctx, opt, ns, db, ix, sdr)?;
```
### 2c. å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œ (`index.rs:172`)
åœ¨åå°ä»»åŠ¡ä¸­æ‰§è¡Œç´¢å¼•æ„å»º
```text
let jh = task::spawn(async move {
			let r = b.run().await;
			if let Err(err) = &r {
				b.set_status(BuildingStatus::Error(err.to_string())).await;
			}
			if let Some(s) = sdr {
				if s.send(r).is_err() {
					warn!("Failed to send index building result to the consumer");
				}
			}
		});
```
### 3. æ–‡æ¡£ç´¢å¼•æ—¶çš„æ¶ˆè´¹æœºåˆ¶
å±•ç¤ºæ–‡æ¡£æ’å…¥æ—¶å¦‚ä½•è¢« IndexBuilder æ¶ˆè´¹ï¼Œå®ç°å¢é‡æ„å»º
### 3a. æ£€æŸ¥ IndexBuilder çŠ¶æ€ (`index.rs:101`)
æ–‡æ¡£ç´¢å¼•æ—¶æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒçš„ç´¢å¼•æ„å»ºè¿‡ç¨‹
```text
let (o, n) = if let Some(ib) = ctx.get_index_builder() {
			match ib.consume(db, ctx, ix, o, n, rid).await? {
				// The index builder consumed the value, which means it is currently building the
				// index asynchronously, we don't index the document and let the index builder
				// do it later.
				ConsumeResult::Enqueued => return Ok(()),
				// The index builder is done, the index has been built; we can proceed normally
				ConsumeResult::Ignored(o, n) => (o, n),
			}
		} else {
			(o, n)
		};
```
### 3b. æ¶ˆè´¹æ–‡æ¡£å˜æ›´ (`index.rs:223`)
IndexBuilder æ¶ˆè´¹æ–‡æ¡£å˜æ›´çš„å…¥å£æ–¹æ³•
```text
pub(crate) async fn consume(
		&self,
		db: &DatabaseDefinition,
		ctx: &Context,
		ix: &IndexDefinition,
		old_values: Option<Vec<Value>>,
		new_values: Option<Vec<Value>>,
		rid: &RecordId,
	) -> Result<ConsumeResult> {
```
### 3c. æŸ¥æ‰¾æ´»è·ƒæ„å»ºä»»åŠ¡ (`index.rs:233`)
æŸ¥æ‰¾å¯¹åº”çš„æ´»è·ƒç´¢å¼•æ„å»ºä»»åŠ¡
```text
if let Some(r) = self.indexes.get(&key) {
			let (b, _) = r.value();
			return b.maybe_consume(ctx, old_values, new_values, rid).await;
		}
```
### 4. å¢é‡æ•°æ®é˜Ÿåˆ—ç®¡ç†
å±•ç¤ºå¦‚ä½•åœ¨ç´¢å¼•æ„å»ºæœŸé—´ç®¡ç†å¢é‡æ•°æ®çš„é˜Ÿåˆ—
### 4a. æ¶ˆè´¹å†³ç­–é€»è¾‘ (`index.rs:369`)
å†³å®šæ˜¯å¦å°†æ–‡æ¡£åŠ å…¥é˜Ÿåˆ—æˆ–ç›´æ¥ç´¢å¼•
```text
async fn maybe_consume(
		&self,
		ctx: &Context,
		old_values: Option<Vec<Value>>,
		new_values: Option<Vec<Value>>,
		rid: &RecordId,
	) -> Result<ConsumeResult> {
```
### 4b. æ£€æŸ¥æ„å»ºçŠ¶æ€ (`index.rs:379`)
å¦‚æœç´¢å¼•å·²æ„å»ºå®Œæˆä¸”é˜Ÿåˆ—ä¸ºç©ºï¼Œç›´æ¥å¤„ç†æ–‡æ¡£
```text
if queue.is_empty() {
			// If the appending queue is empty and the index is built...
			if self.status.read().await.is_ready() {
				// ... we return the values back, so the document can be updated the usual way
				return Ok(ConsumeResult::Ignored(old_values, new_values));
			}
		}
```
### 4c. åŠ å…¥å¢é‡é˜Ÿåˆ— (`index.rs:388`)
å°†æ–‡æ¡£å˜æ›´å­˜å‚¨åˆ°å¢é‡é˜Ÿåˆ—ä¸­
```text
let a = Appending {
			old_values,
			new_values,
			id: rid.key.clone(),
		};
		// Get the idx of this appended record from the sequence
		let idx = queue.add_update();
		// Store the appending
		let ia = self.ikb.new_ia_key(idx);
		tx.set(&ia, &a, None).await?;
```
### 5. HNSW ç´¢å¼•çš„ä¸¤é˜¶æ®µæ„å»º
å±•ç¤º HNSW ç´¢å¼•å¦‚ä½•å…ˆå¤„ç†åˆå§‹æ•°æ®ï¼Œå†å¤„ç†å¢é‡æ›´æ–°
### 5a. å¼€å§‹æ„å»ºæµç¨‹ (`index.rs:419`)
æ¸…ç†æ—§ç´¢å¼•æ•°æ®ï¼Œå¼€å§‹æ„å»ºè¿‡ç¨‹
```text
async fn run(&self) -> Result<()> {
		// Remove the index data
		{
			self.set_status(BuildingStatus::Cleaning).await;
```
### 5b. åˆå§‹æ•°æ®ç´¢å¼• (`index.rs:432`)
ç¬¬ä¸€é˜¶æ®µï¼šç´¢å¼•è¡¨ä¸­æ‰€æœ‰ç°æœ‰æ•°æ®
```text
// First iteration, we index every key
		let beg = thing::prefix(self.ns, self.db, self.ikb.table())?;
		let end = thing::suffix(self.ns, self.db, self.ikb.table())?;
```
### 5c. å¢é‡æ•°æ®å¤„ç† (`index.rs:472`)
ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†æ„å»ºæœŸé—´çš„å¢é‡å˜æ›´
```text
// Second iteration, we index/remove any records that has been added or removed
		// since the initial indexing
```
### 6. HNSW å‘é‡æ’å…¥æœºåˆ¶
å±•ç¤º HNSW ç´¢å¼•å¦‚ä½•åŒæ—¶æ›´æ–°å‘é‡è¡¨å’Œå›¾ç»“æ„
### 6a. HNSW æ–‡æ¡£ç´¢å¼• (`index.rs:91`)
HNSW ç´¢å¼•çš„æ–‡æ¡£ç´¢å¼•å…¥å£
```text
pub async fn index_document(
		&mut self,
		tx: &Transaction,
		id: &RecordIdKey,
		content: &[Value],
	) -> Result<()> {
```
### 6b. æ’å…¥å‘é‡æ•°æ® (`index.rs:107`)
åŒæ—¶æ›´æ–°å‘é‡è¡¨å’Œ HNSW å›¾ç»“æ„
```text
self.vec_docs.insert(tx, vector, doc_id, &mut self.hnsw).await?;
```
### 6c. æ„å»º HNSW å›¾ (`docs.rs:188`)
åœ¨ HNSW å›¾ä¸­æ’å…¥æ–°å‘é‡èŠ‚ç‚¹
```text
None => {
				//  We don't have the vector, we insert it in the graph
				let element_id = h.insert(tx, o).await?;
```

# DeepWiki Q&A with Code Context for Repository: surrealdb/surrealdb
## Q1
ä½ ç»™æˆ‘æŸ¥è¯¢ä¸‹
Surrealdbæä¾›å“ªäº›å‘é‡æœç´¢çš„æ–¹æ³•ï¼Ÿ
å¦‚æœæˆ‘ä¸€ä¸ªæ•°æ®åº“é‡Œé¢æœ‰å¤šä¸ªå‘é‡è¡¨ ä¸ºä¸åŒçš„ç»´åº¦å’Œæ¨¡å‹ å¦‚ä½•å•ç‹¬çš„æœç´¢ï¼Ÿå¦‚ä½•æ··åˆæœç´¢ï¼Ÿ
### Answer
# SurrealDB å‘é‡æœç´¢æ–¹æ³•

## ä¸€ã€å‘é‡æœç´¢æ–¹æ³•

SurrealDB æä¾›äº†ä»¥ä¸‹å‡ ç§å‘é‡æœç´¢æ–¹æ³•ï¼š

### 1. **HNSWï¼ˆè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼‰**
ä½¿ç”¨ `<|k,ef|>` æ“ä½œç¬¦è¿›è¡Œå¿«é€Ÿçš„è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ã€‚å…¶ä¸­ `k` æ˜¯è¿”å›çš„é‚»å±…æ•°é‡ï¼Œ`ef` æ˜¯æœç´¢è´¨é‡å‚æ•°ã€‚ surrealdb:1-50 

**ç´¢å¼•å®šä¹‰è¯­æ³•ï¼š**
```surql
DEFINE INDEX idx_name ON table FIELDS embedding HNSW DIMENSION 3 DIST COSINE;
``` surrealdb:49-50 

### 2. **M-Treeï¼ˆç²¾ç¡®æœ€è¿‘é‚»æœç´¢ï¼‰**
ä½¿ç”¨ `<|k|>` æ“ä½œç¬¦è¿›è¡Œç²¾ç¡®çš„ k-è¿‘é‚»æœç´¢ï¼ŒåŸºäºè·ç¦»çš„æ ‘çŠ¶ç´¢å¼•ç»“æ„ã€‚ surrealdb:1-50 

**ç´¢å¼•å®šä¹‰è¯­æ³•ï¼š**
```surql
DEFINE INDEX idx_name ON table FIELDS point MTREE DIMENSION 4 TYPE F32;
``` surrealdb:14-14 

### 3. **æš´åŠ›æœç´¢ï¼ˆBrute-forceï¼‰**
ä½¿ç”¨ `<|k,DISTANCE|>` æ“ä½œç¬¦è¿›è¡Œç²¾ç¡®æœç´¢ï¼Œä¸éœ€è¦ç´¢å¼•ï¼Œé€‚åˆå°æ•°æ®é›†ã€‚ surrealdb:565-570 

### 4. **æ”¯æŒçš„è·ç¦»åº¦é‡**

SurrealDB æ”¯æŒä»¥ä¸‹è·ç¦»åº¦é‡æ–¹æ³•ï¼š
- **Euclidean**ï¼ˆæ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œé»˜è®¤ï¼‰
- **Cosine**ï¼ˆä½™å¼¦è·ç¦»ï¼‰
- **Manhattan**ï¼ˆæ›¼å“ˆé¡¿è·ç¦»ï¼‰
- **Chebyshev**ï¼ˆåˆ‡æ¯”é›ªå¤«è·ç¦»ï¼‰
- **Hamming**ï¼ˆæ±‰æ˜è·ç¦»ï¼‰
- **Jaccard**ï¼ˆæ°å¡å¾·ç›¸ä¼¼åº¦ï¼‰
- **Minkowski**ï¼ˆé—µå¯å¤«æ–¯åŸºè·ç¦»ï¼‰
- **Pearson**ï¼ˆçš®å°”é€Šç›¸å…³ç³»æ•°ï¼‰ surrealdb:217-227 

## äºŒã€å•ç‹¬æœç´¢ä¸åŒç»´åº¦çš„å‘é‡è¡¨

### ä¸ºæ¯ä¸ªè¡¨å®šä¹‰ç‹¬ç«‹çš„å‘é‡ç´¢å¼•

æ¯ä¸ªè¡¨å¯ä»¥å®šä¹‰è‡ªå·±çš„å‘é‡ç´¢å¼•ï¼ŒæŒ‡å®šä¸åŒçš„ç»´åº¦å’Œè·ç¦»åº¦é‡ï¼š

**ç¤ºä¾‹ï¼š**
```surql
-- è¡¨1ï¼š3ç»´å‘é‡ï¼Œä½™å¼¦è·ç¦»
DEFINE INDEX idx_table1 ON table1 FIELDS embedding HNSW DIMENSION 3 DIST COSINE;

-- è¡¨2ï¼š128ç»´å‘é‡ï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»
DEFINE INDEX idx_table2 ON table2 FIELDS embedding HNSW DIMENSION 128 DIST EUCLIDEAN;

-- è¡¨3ï¼š768ç»´å‘é‡ï¼Œæ›¼å“ˆé¡¿è·ç¦»
DEFINE INDEX idx_table3 ON table3 FIELDS embedding MTREE DIMENSION 768 DIST MANHATTAN;
```

### æŸ¥è¯¢è¯­æ³•

å¯¹æ¯ä¸ªè¡¨è¿›è¡Œå•ç‹¬çš„å‘é‡æœç´¢ï¼š

```surql
-- åœ¨ table1 ä¸­æœç´¢ï¼ˆ3ç»´å‘é‡ï¼‰
LET $vec3d = [0.1, 0.2, 0.3];
SELECT id, vector::distance::knn() as distance 
FROM table1 
WHERE embedding <|5,100|> $vec3d;

-- åœ¨ table2 ä¸­æœç´¢ï¼ˆ128ç»´å‘é‡ï¼‰
LET $vec128d = [...]; -- 128ç»´å‘é‡
SELECT id, vector::distance::knn() as distance 
FROM table2 
WHERE embedding <|5,100|> $vec128d;
``` surrealdb:53-56 surrealdb:79-101 

## ä¸‰ã€æ··åˆæœç´¢ï¼ˆè·¨è¡¨èåˆï¼‰

SurrealDB æä¾›äº†ä¸¤ç§ç»“æœèåˆæ–¹æ³•æ¥ç»„åˆä¸åŒè¡¨çš„æœç´¢ç»“æœï¼š

### 1. **RRFï¼ˆReciprocal Rank Fusionï¼‰**

ä½¿ç”¨ `search::rrf()` å‡½æ•°è¿›è¡Œå€’æ•°æ’åèåˆï¼š

```surql
-- ä»å¤šä¸ªè¡¨æœç´¢
LET $results_table1 = SELECT id, vector::distance::knn() as distance 
                      FROM table1 WHERE embedding <|5|> $vector1;
                      
LET $results_table2 = SELECT id, vector::distance::knn() as distance 
                      FROM table2 WHERE embedding <|5|> $vector2;

-- ä½¿ç”¨ RRF èåˆç»“æœ
RETURN search::rrf([$results_table1, $results_table2], 10, 60);
```

**å‚æ•°è¯´æ˜ï¼š**
- ç¬¬ä¸€ä¸ªå‚æ•°ï¼šç»“æœæ•°ç»„åˆ—è¡¨
- ç¬¬äºŒä¸ªå‚æ•°ï¼šè¿”å›çš„æœ€å¤§ç»“æœæ•°
- ç¬¬ä¸‰ä¸ªå‚æ•°ï¼šRRF å¸¸æ•°ï¼ˆé»˜è®¤ 60ï¼‰ surrealdb:103-141 surrealdb:53-60 

### 2. **Linearï¼ˆåŠ æƒçº¿æ€§ç»„åˆï¼‰**

ä½¿ç”¨ `search::linear()` å‡½æ•°è¿›è¡ŒåŠ æƒçº¿æ€§ç»„åˆï¼š

```surql
-- ä»å¤šä¸ªè¡¨æœç´¢
LET $results_table1 = SELECT id, vector::distance::knn() as distance 
                      FROM table1 WHERE embedding <|5|> $vector1;
                      
LET $results_table2 = SELECT id, vector::distance::knn() as distance 
                      FROM table2 WHERE embedding <|5|> $vector2;

-- ä½¿ç”¨åŠ æƒçº¿æ€§ç»„åˆèåˆç»“æœï¼ˆtable1 æƒé‡ 2.0ï¼Œtable2 æƒé‡ 1.0ï¼‰
RETURN search::linear([$results_table1, $results_table2], [2.0, 1.0], 10, 'minmax');
```

**å‚æ•°è¯´æ˜ï¼š**
- ç¬¬ä¸€ä¸ªå‚æ•°ï¼šç»“æœæ•°ç»„åˆ—è¡¨
- ç¬¬äºŒä¸ªå‚æ•°ï¼šæƒé‡æ•°ç»„ï¼ˆå¯¹åº”æ¯ä¸ªç»“æœåˆ—è¡¨ï¼‰
- ç¬¬ä¸‰ä¸ªå‚æ•°ï¼šè¿”å›çš„æœ€å¤§ç»“æœæ•°
- ç¬¬å››ä¸ªå‚æ•°ï¼šå½’ä¸€åŒ–æ–¹æ³•ï¼ˆ'minmax' æˆ– 'zscore'ï¼‰

**å½’ä¸€åŒ–æ–¹æ³•ï¼š**
- **MinMax**ï¼šå°†åˆ†æ•°ç¼©æ”¾åˆ° [0,1] èŒƒå›´
- **Z-score**ï¼šä½¿ç”¨æ ‡å‡†åŒ–å¤„ç† surrealdb:269-332 surrealdb:62-66 

### 3. **åˆ†æ•°æå–è§„åˆ™**

èåˆå‡½æ•°ä¼šè‡ªåŠ¨ä»æ–‡æ¡£ä¸­æå–åˆ†æ•°ï¼Œä¼˜å…ˆçº§å¦‚ä¸‹ï¼š
1. `distance` å­—æ®µ - ä½¿ç”¨ `1.0 / (1.0 + distance)` è½¬æ¢ï¼ˆè·ç¦»è¶Šå°ï¼Œåˆ†æ•°è¶Šé«˜ï¼‰
2. `ft_score` å­—æ®µ - å…¨æ–‡æœç´¢åˆ†æ•°
3. `score` å­—æ®µ - é€šç”¨åˆ†æ•°
4. åŸºäºæ’åçš„å›é€€ - `1.0 / (1.0 + rank)` surrealdb:306-314 

## Notes

**å…³é”®ç‚¹ï¼š**

1. **ç»´åº¦ç‹¬ç«‹æ€§**ï¼šæ¯ä¸ªè¡¨çš„å‘é‡ç´¢å¼•å¯ä»¥æœ‰ä¸åŒçš„ç»´åº¦ï¼Œç»´åº¦åœ¨ç´¢å¼•å®šä¹‰æ—¶é€šè¿‡ `DIMENSION` å‚æ•°æŒ‡å®šã€‚å‘é‡æœç´¢å¿…é¡»ä½¿ç”¨ä¸ç´¢å¼•å®šä¹‰ç›¸åŒç»´åº¦çš„æŸ¥è¯¢å‘é‡ã€‚

2. **èåˆçº§åˆ«**ï¼šæœç´¢èåˆå‘ç”Ÿåœ¨ç»“æœçº§åˆ«ï¼ˆåˆå¹¶æ’ååˆ—è¡¨ï¼‰ï¼Œè€Œä¸æ˜¯å‘é‡çº§åˆ«ï¼Œå› æ­¤ä¸åŒç»´åº¦çš„è¡¨å¯ä»¥å®‰å…¨åœ°èåˆç»“æœã€‚

3. **ç´¢å¼•ç±»å‹é€‰æ‹©**ï¼š
   - HNSW é€‚åˆå¤§è§„æ¨¡æ•°æ®çš„è¿‘ä¼¼æœç´¢ï¼ˆé€Ÿåº¦å¿«ï¼‰
   - M-Tree é€‚åˆç²¾ç¡®æœç´¢ï¼ˆç»“æœå‡†ç¡®ï¼‰
   - æš´åŠ›æœç´¢é€‚åˆå°æ•°æ®é›†æˆ–æ²¡æœ‰ç´¢å¼•çš„æƒ…å†µ

4. **å‘é‡ç±»å‹æ”¯æŒ**ï¼šç´¢å¼•æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼šF64ã€F32ã€I64ã€I32ã€I16ï¼Œé€šè¿‡ `TYPE` å‚æ•°æŒ‡å®šã€‚ surrealdb:437-467
### Citations
**File:** crates/core/src/idx/trees/hnsw/index.rs (L1-50)
```rust
use std::collections::VecDeque;

#[cfg(debug_assertions)]
use ahash::HashMap;
use anyhow::Result;
use reblessive::tree::Stk;

use crate::catalog::{DatabaseDefinition, HnswParams, VectorType};
use crate::idx::IndexKeyBase;
use crate::idx::planner::checker::HnswConditionChecker;
use crate::idx::planner::iterators::KnnIteratorResult;
use crate::idx::trees::hnsw::docs::{HnswDocs, VecDocs};
use crate::idx::trees::hnsw::elements::HnswElements;
use crate::idx::trees::hnsw::flavor::HnswFlavor;
use crate::idx::trees::hnsw::{ElementId, HnswSearch};
use crate::idx::trees::knn::{KnnResult, KnnResultBuilder};
use crate::idx::trees::vector::{SharedVector, Vector};
use crate::kvs::Transaction;
use crate::val::{Number, RecordIdKey, Value};

pub struct HnswIndex {
	dim: usize,
	vector_type: VectorType,
	hnsw: HnswFlavor,
	docs: HnswDocs,
	vec_docs: VecDocs,
}

pub(super) struct HnswCheckedSearchContext<'a> {
	elements: &'a HnswElements,
	docs: &'a HnswDocs,
	vec_docs: &'a VecDocs,
	pt: &'a SharedVector,
	ef: usize,
}

impl<'a> HnswCheckedSearchContext<'a> {
	pub(super) fn new(
		elements: &'a HnswElements,
		docs: &'a HnswDocs,
		vec_docs: &'a VecDocs,
		pt: &'a SharedVector,
		ef: usize,
	) -> Self {
		Self {
			elements,
			docs,
			vec_docs,
			pt,
			ef,
```
**File:** crates/language-tests/tests/language/functions/search/search-linear.surql (L49-50)
```text
-- Vector index (HNSW) on a 3â€‘dimensional embedding, using cosine distance
DEFINE INDEX idx_embedding ON TABLE test FIELDS embedding HNSW DIMENSION 3 DIST COSINE;
```
**File:** crates/language-tests/tests/language/functions/search/search-linear.surql (L53-56)
```text
LET $qvec = [0.12, 0.18, 0.27];

-- Vector search: top 2 nearest neighbours
LET $vs = SELECT id, vector::distance::knn() as distance FROM test WHERE embedding <|2,100|> $qvec;
```
**File:** crates/language-tests/tests/language/functions/search/search-linear.surql (L62-66)
```text
-- Fuse with Linear / minmax
RETURN search::linear([$vs, $ft], [2, 1], 2, 'minmax');

-- Fuse with Linear / zscore
RETURN search::linear([$vs, $ft], [2, 1], 2, 'zscore');
```
**File:** crates/core/src/idx/trees/mtree.rs (L1-50)
```rust
use std::collections::hash_map::Entry;
use std::collections::{BinaryHeap, VecDeque};
use std::fmt::{Debug, Display, Formatter};
use std::io::Cursor;
use std::sync::Arc;

use ahash::{HashMap, HashMapExt, HashSet};
use anyhow::Result;
use reblessive::tree::Stk;
use revision::{Revisioned, revisioned};
use roaring::RoaringTreemap;
use serde::{Deserialize, Serialize};
use tokio::sync::RwLock;

use crate::catalog::{DatabaseDefinition, Distance, MTreeParams, VectorType};
use crate::ctx::Context;
use crate::err::Error;
use crate::idx::IndexKeyBase;
use crate::idx::docids::DocId;
use crate::idx::docids::btdocids::BTreeDocIds;
use crate::idx::planner::checker::MTreeConditionChecker;
use crate::idx::planner::iterators::KnnIteratorResult;
use crate::idx::trees::btree::BStatistics;
use crate::idx::trees::knn::{Ids64, KnnResult, KnnResultBuilder, PriorityNode};
use crate::idx::trees::store::{NodeId, StoredNode, TreeNode, TreeNodeProvider, TreeStore};
use crate::idx::trees::vector::{SharedVector, Vector};
use crate::kvs::{KVValue, Key, Transaction, TransactionType, Val};
use crate::val::{Number, Object, RecordId, Value};

pub struct MTreeIndex {
	ikb: IndexKeyBase,
	dim: usize,
	vector_type: VectorType,
	store: MTreeStore,
	doc_ids: Arc<RwLock<BTreeDocIds>>,
	mtree: Arc<RwLock<MTree>>,
}

struct MTreeSearchContext<'a> {
	ctx: &'a Context,
	pt: SharedVector,
	k: usize,
	store: &'a MTreeStore,
}

impl MTreeIndex {
	pub async fn new(
		txn: &Transaction,
		ikb: IndexKeyBase,
		p: &MTreeParams,
```
**File:** crates/sdk/tests/vector.rs (L14-14)
```rust
		DEFINE INDEX mt_pts ON pts FIELDS point MTREE DIMENSION 4 TYPE F32;
```
**File:** crates/core/src/idx/planner/tree.rs (L565-570)
```rust
	fn eval_bruteforce_knn(&mut self, id: &Idiom, val: &Node, exp: &Arc<Expr>) -> Result<()> {
		let Expr::Binary {
			op,
			..
		} = &**exp
		else {
```
**File:** crates/core/src/sql/index.rs (L217-227)
```rust
pub enum Distance {
	Chebyshev,
	Cosine,
	#[default]
	Euclidean,
	Hamming,
	Jaccard,
	Manhattan,
	Minkowski(Number),
	Pearson,
}
```
**File:** crates/core/src/fnc/vector.rs (L79-101)
```rust
	pub fn knn(
		(ctx, doc): (&Context, Option<&CursorDoc>),
		(Optional(knn_ref),): (Optional<Value>,),
	) -> Result<Value> {
		if let Some((_exe, doc, thg)) = get_execution_context(ctx, doc) {
			if let Some(ir) = &doc.ir {
				if let Some(d) = ir.dist() {
					return Ok(d.into());
				}
			}
			if let Some(IterationStage::Iterate(Some(results))) = ctx.get_iteration_stage() {
				let n = if let Some(Value::Number(n)) = knn_ref {
					n.as_usize()
				} else {
					0
				};
				if let Some(d) = results.get_dist(n, thg) {
					return Ok(d.into());
				}
			}
		}
		Ok(Value::None)
	}
```
**File:** crates/core/src/fnc/search.rs (L103-141)
```rust
/// Implements Reciprocal Rank Fusion (RRF) to combine multiple ranked result
/// lists.
///
/// RRF is a method for combining results from different search algorithms
/// (e.g., vector search and full-text search) by computing a unified score
/// based on the reciprocal of each document's rank in each result list. The
/// algorithm uses the formula: `1 / (k + rank)` where `k` is the RRF constant
/// and `rank` is the 1-based position in the result list.
///
/// # Parameters
///
/// * `ctx` - The execution context for cancellation checking and transaction management
/// * `results` - An array of result lists, where each list contains documents with an "id" field
/// * `limit` - Maximum number of documents to return (must be â‰¥ 1)
/// * `rrf_constant` - Optional RRF constant (k) for score calculation (defaults to 60.0, must be â‰¥
///   0)
///
/// # Returns
///
/// Returns a `Value::Array` containing the top `limit` documents sorted by RRF
/// score in descending order. Each document includes:
/// - All original fields from the input documents (merged if the same document appears in multiple
///   lists)
/// - `id`: The document identifier
/// - `rrf_score`: The computed RRF score as a float
///
/// # Errors
///
/// * `Error::InvalidArguments` - If `limit` < 1 or `rrf_constant` < 0
/// * Context cancellation errors if the operation is cancelled during processing
///
/// # Example
///
/// ```surql
/// -- Combine vector search and full-text search results
/// LET $vector_results = SELECT id, distance FROM docs WHERE embedding <|5|> $query_vector;
/// LET $text_results = SELECT id, ft_score FROM docs WHERE text @@ 'search terms';
/// RETURN search::rrf([$vector_results, $text_results], 10, 60);
/// ```
```
**File:** crates/core/src/fnc/search.rs (L269-332)
```rust
/// Implements weighted linear combination to fuse multiple ranked result lists.
///
/// Linear combination is a method for combining results from different search
/// algorithms (e.g., vector search and full-text search) by computing a unified
/// score based on weighted linear combination of normalized scores.
/// The algorithm first normalizes scores from each result list using either
/// MinMax or Z-score normalization, then computes a weighted sum: `weightâ‚ Ã—
/// norm_scoreâ‚ + weightâ‚‚ Ã— norm_scoreâ‚‚ + ...`
///
/// # Parameters
///
/// * `ctx` - The execution context for cancellation checking and transaction management
/// * `results` - An array of result lists, where each list contains documents with an "id" field
/// * `weights` - An array of numeric weights corresponding to each result list (must have same
///   length as results)
/// * `limit` - Maximum number of documents to return (must be â‰¥ 1)
/// * `norm` - Normalization method: "minmax" for MinMax normalization or "zscore" for Z-score
///   normalization
///
/// # Returns
///
/// Returns a `Value::Array` containing the top `limit` documents sorted by
/// linear score in descending order. Each document includes:
/// - All original fields from the input documents (merged if the same document appears in multiple
///   lists)
/// - `id`: The document identifier
/// - `linear_score`: The computed weighted linear combination score as a float
///
/// # Errors
///
/// * `Error::InvalidArguments` - If:
///   - `limit` < 1
///   - `results` and `weights` arrays have different lengths
///   - Any weight is not a numeric value
///   - `norm` is not "minmax" or "zscore"
/// * Context cancellation errors if the operation is cancelled during processing
///
/// # Score Extraction
///
/// The function automatically extracts scores from documents using the
/// following priority:
/// 1. `distance` field - converted using `1.0 / (1.0 + distance)` (lower distance = higher score)
/// 2. `ft_score` field - used directly (full-text search scores)
/// 3. `score` field - used directly (generic scores)
/// 4. Rank-based fallback - `1.0 / (1.0 + rank)` if no score field is found
///
/// # Normalization Methods
///
/// * **MinMax**: Scales scores to [0,1] range using `(score - min) / (max - min)`
/// * **Z-score**: Standardizes scores using `(score - mean) / std_dev`
///
/// # Example
///
/// ```surql
/// -- Combine vector search and full-text search results with different weights
/// LET $vector_results = SELECT id, distance FROM docs WHERE embedding <|5|> $query_vector;
/// LET $text_results = SELECT id, ft_score FROM docs WHERE text @@ 'search terms';
///
/// -- Use MinMax normalization with 2:1 weighting favoring vector search
/// RETURN search::linear([$vector_results, $text_results], [2.0, 1.0], 10, 'minmax');
///
/// -- Use Z-score normalization with equal weighting
/// RETURN search::linear([$vector_results, $text_results], [1.0, 1.0], 10, 'zscore');
/// ```
```
**File:** crates/language-tests/tests/language/functions/search/search-rrf.surql (L53-60)
```text
LET $vs = SELECT id, vector::distance::knn() as distance FROM test WHERE embedding <|2,100|> $qvec;

-- Fullâ€‘text search: top 2 lexical matches
LET $ft = SELECT id, search::score(1) as ft_score FROM test
          WHERE text @1@ 'graph' ORDER BY ft_score DESC LIMIT 2;

-- Fuse with Reciprocal Rank Fusion (k defaults to 60 if omitted)
RETURN search::rrf([$vs, $ft], 2, 60);
```
**File:** crates/core/src/idx/trees/vector.rs (L437-467)
```rust
impl Vector {
	pub(super) fn try_from_value(t: VectorType, d: usize, v: &Value) -> Result<Self> {
		let res = match t {
			VectorType::F64 => {
				let mut vec = Vec::with_capacity(d);
				Self::check_vector_value(v, &mut vec)?;
				Vector::F64(Array1::from_vec(vec))
			}
			VectorType::F32 => {
				let mut vec = Vec::with_capacity(d);
				Self::check_vector_value(v, &mut vec)?;
				Vector::F32(Array1::from_vec(vec))
			}
			VectorType::I64 => {
				let mut vec = Vec::with_capacity(d);
				Self::check_vector_value(v, &mut vec)?;
				Vector::I64(Array1::from_vec(vec))
			}
			VectorType::I32 => {
				let mut vec = Vec::with_capacity(d);
				Self::check_vector_value(v, &mut vec)?;
				Vector::I32(Array1::from_vec(vec))
			}
			VectorType::I16 => {
				let mut vec = Vec::with_capacity(d);
				Self::check_vector_value(v, &mut vec)?;
				Vector::I16(Array1::from_vec(vec))
			}
		};
		Ok(res)
	}
```
## Q2
ä½ ç ”ç©¶ä¸‹ è¿™ä¸ªhjnswç´¢å¼•èƒ½ä¸èƒ½å°†å‘é‡åŠ å…¥å‘é‡è¡¨ä¸å¢é‡æ„å»ºhnswçš„è¿‡ç¨‹åˆ†å¼€ï¼Ÿ
### Answer
åŸºäºå¯¹ SurrealDB ä»£ç åº“çš„åˆ†æï¼ŒHNSW ç´¢å¼•ç›®å‰çš„è®¾è®¡æ˜¯å°†å‘é‡åŠ å…¥å‘é‡è¡¨ä¸å¢é‡æ„å»º HNSW çš„è¿‡ç¨‹ç´§å¯†è€¦åˆçš„ï¼Œä½†ç¡®å®å­˜åœ¨ä¸€äº›åˆ†ç¦»æœºåˆ¶ã€‚

## å½“å‰æ¶æ„åˆ†æ

### ç´§è€¦åˆçš„è®¾è®¡

åœ¨ `HnswIndex.index_document()` æ–¹æ³•ä¸­ï¼Œå‘é‡æ’å…¥å’Œ HNSW æ„å»ºæ˜¯åŒæ­¥è¿›è¡Œçš„ï¼š surrealdb:91-111 

```rust
pub async fn index_document(
    &mut self,
    tx: &Transaction,
    id: &RecordIdKey,
    content: &[Value],
) -> Result<()> {
    // ç¡®ä¿å±‚çº§æ˜¯æœ€æ–°çš„
    self.hnsw.check_state(tx).await?;
    // è§£ææ–‡æ¡£ID
    let doc_id = self.docs.resolve(tx, id).await?;
    // ç´¢å¼•å€¼
    for value in content.iter().filter(|v| !v.is_nullish()) {
        // æå–å‘é‡
        let vector = Vector::try_from_value(self.vector_type, self.dim, value)?;
        vector.check_dimension(self.dim)?;
        // æ’å…¥å‘é‡ - è¿™é‡ŒåŒæ—¶æ›´æ–°å‘é‡è¡¨å’ŒHNSWå›¾
        self.vec_docs.insert(tx, vector, doc_id, &mut self.hnsw).await?;
    }
    self.docs.finish(tx).await?;
    Ok(())
}
```

### ä¸¤å±‚å­˜å‚¨ç»“æ„

ç³»ç»Ÿå®é™…ä¸Šä½¿ç”¨äº†ä¸¤å±‚å­˜å‚¨ï¼š

1. **VecDocs** - ç®¡ç†å‘é‡åˆ°æ–‡æ¡£IDçš„æ˜ å°„ surrealdb:170-200 
2. **HnswFlavor** - ç®¡ç†å®é™…çš„ HNSW å›¾ç»“æ„ surrealdb:76-93 

åœ¨ `VecDocs.insert()` æ–¹æ³•ä¸­ï¼Œå¦‚æœå‘é‡ä¸å­˜åœ¨ï¼Œä¼šè°ƒç”¨ `h.insert(tx, o).await?` æ¥æ„å»º HNSW å›¾ï¼š surrealdb:188-194 

## å¼‚æ­¥ç´¢å¼•æ„å»ºæœºåˆ¶

SurrealDB ç¡®å®æä¾›äº†å¼‚æ­¥ç´¢å¼•æ„å»ºçš„èƒ½åŠ›ï¼Œé€šè¿‡ `IndexBuilder` ç»„ä»¶ï¼š surrealdb:147-184 

åœ¨æ–‡æ¡£ç´¢å¼•è¿‡ç¨‹ä¸­ï¼Œå¦‚æœå­˜åœ¨æ´»è·ƒçš„ç´¢å¼•æ„å»ºå™¨ï¼Œæ–‡æ¡£ç´¢å¼•æ“ä½œä¼šè¢«è·³è¿‡ï¼Œç”±ç´¢å¼•æ„å»ºå™¨å¼‚æ­¥å¤„ç†ï¼š surrealdb:100-112 

```rust
let (o, n) = if let Some(ib) = ctx.get_index_builder() {
    match ib.consume(db, ctx, ix, o, n, rid).await? {
        // ç´¢å¼•æ„å»ºå™¨æ¶ˆè´¹äº†å€¼ï¼Œæ„å‘³ç€å®ƒæ­£åœ¨å¼‚æ­¥æ„å»ºç´¢å¼•
        // æˆ‘ä»¬ä¸ç´¢å¼•æ–‡æ¡£ï¼Œè®©ç´¢å¼•æ„å»ºå™¨ç¨åå¤„ç†
        ConsumeResult::Enqueued => return Ok(()),
        // ç´¢å¼•æ„å»ºå™¨å®Œæˆï¼Œç´¢å¼•å·²æ„å»ºï¼›æˆ‘ä»¬å¯ä»¥æ­£å¸¸è¿›è¡Œ
        ConsumeResult::Ignored(o, n) => (o, n),
    }
} else {
    (o, n)
};
```

## ç»“è®º

**ç›®å‰ HNSW ç´¢å¼•çš„è®¾è®¡ç‰¹ç‚¹ï¼š**

1. **é»˜è®¤æƒ…å†µä¸‹**ï¼šå‘é‡æ’å…¥å’Œ HNSW æ„å»ºæ˜¯åŒæ­¥çš„ã€ç´§å¯†è€¦åˆçš„è¿‡ç¨‹
2. **å¼‚æ­¥æ„å»ºæ”¯æŒ**ï¼šé€šè¿‡ `IndexBuilder` å¯ä»¥å®ç°å¼‚æ­¥ç´¢å¼•æ„å»ºï¼Œå°†å‘é‡æ•°æ®æ”¶é›†å’Œ HNSW å›¾æ„å»ºåˆ†ç¦»
3. **ä¸¤å±‚å­˜å‚¨**ï¼šå‘é‡æ•°æ®å­˜å‚¨åœ¨ `VecDocs` ä¸­ï¼ŒHNSW å›¾ç»“æ„å­˜å‚¨åœ¨ `HnswFlavor` ä¸­ï¼Œä¸ºåˆ†ç¦»æä¾›äº†åŸºç¡€æ¶æ„

**è¦å®ç°å®Œå…¨åˆ†ç¦»**ï¼Œä½ éœ€è¦ä½¿ç”¨å¼‚æ­¥ç´¢å¼•æ„å»ºåŠŸèƒ½ï¼Œè¿™æ ·å‘é‡æ•°æ®å¯ä»¥å…ˆå­˜å‚¨åˆ° `VecDocs` ä¸­ï¼Œç„¶åç”±åå°çš„ `IndexBuilder` å¼‚æ­¥æ„å»º HNSW å›¾ç»“æ„ã€‚

## Notes

- HNSW çš„å¢é‡æ„å»ºæ˜¯é€šè¿‡ `insert_level()` æ–¹æ³•å®ç°çš„ï¼Œå®ƒä¼šåŠ¨æ€åˆ›å»ºæ–°çš„å±‚çº§å¹¶æ›´æ–°å›¾è¿æ¥ surrealdb:137-167 
- ç³»ç»Ÿæ”¯æŒå¤šç§ HNSW "flavor"ï¼Œæ ¹æ®ä¸åŒçš„ M å’Œ M0 å‚æ•°é€‰æ‹©ä¸åŒçš„æ•°æ®ç»“æ„ä¼˜åŒ– surrealdb:31-55 
- å‘é‡åˆ é™¤æ“ä½œä¹Ÿä¼šåŒæ—¶æ›´æ–°å‘é‡è¡¨å’Œ HNSW å›¾ surrealdb:202-222 

Wiki pages you might want to explore:
- [Core Database Engine (surrealdb/surrealdb)](/wiki/surrealdb/surrealdb#6)
### Citations
**File:** crates/core/src/idx/trees/hnsw/index.rs (L91-111)
```rust
	pub async fn index_document(
		&mut self,
		tx: &Transaction,
		id: &RecordIdKey,
		content: &[Value],
	) -> Result<()> {
		// Ensure the layers are up-to-date
		self.hnsw.check_state(tx).await?;
		// Resolve the doc_id
		let doc_id = self.docs.resolve(tx, id).await?;
		// Index the values
		for value in content.iter().filter(|v| !v.is_nullish()) {
			// Extract the vector
			let vector = Vector::try_from_value(self.vector_type, self.dim, value)?;
			vector.check_dimension(self.dim)?;
			// Insert the vector
			self.vec_docs.insert(tx, vector, doc_id, &mut self.hnsw).await?;
		}
		self.docs.finish(tx).await?;
		Ok(())
	}
```
**File:** crates/core/src/idx/trees/hnsw/docs.rs (L170-200)
```rust
	pub(super) async fn insert(
		&self,
		tx: &Transaction,
		o: Vector,
		d: DocId,
		h: &mut HnswFlavor,
	) -> Result<()> {
		let ser_vec = Arc::new(SerializedVector::from(&o));
		let key = self.ikb.new_hv_key(ser_vec);
		if let Some(ed) = match tx.get(&key, None).await? {
			Some(mut ed) => {
				// We already have the vector
				ed.docs.insert(d).map(|new_docs| {
					ed.docs = new_docs;
					ed
				})
			}
			None => {
				//  We don't have the vector, we insert it in the graph
				let element_id = h.insert(tx, o).await?;
				let ed = ElementDocs {
					e_id: element_id,
					docs: Ids64::One(d),
				};
				Some(ed)
			}
		} {
			tx.set(&key, &ed, None).await?;
		}
		Ok(())
	}
```
**File:** crates/core/src/idx/trees/hnsw/docs.rs (L202-222)
```rust
	pub(super) async fn remove(
		&self,
		tx: &Transaction,
		o: &Vector,
		d: DocId,
		h: &mut HnswFlavor,
	) -> Result<()> {
		let key = self.ikb.new_hv_key(Arc::new(o.into()));
		if let Some(mut ed) = tx.get(&key, None).await? {
			if let Some(new_docs) = ed.docs.remove(d) {
				if new_docs.is_empty() {
					tx.del(&key).await?;
					h.remove(tx, ed.e_id).await?;
				} else {
					ed.docs = new_docs;
					tx.set(&key, &ed, None).await?;
				}
			}
		};
		Ok(())
	}
```
**File:** crates/core/src/idx/trees/hnsw/flavor.rs (L31-55)
```rust
	pub(super) fn new(ibk: IndexKeyBase, p: &HnswParams) -> Result<Self> {
		let res = match p.m {
			1..=4 => match p.m0 {
				1..=8 => Self::H5_9(Hnsw::<ArraySet<9>, ArraySet<5>>::new(ibk, p)?),
				9..=16 => Self::H5_17(Hnsw::<ArraySet<17>, ArraySet<5>>::new(ibk, p)?),
				17..=24 => Self::H5_25(Hnsw::<ArraySet<25>, ArraySet<5>>::new(ibk, p)?),
				_ => Self::H5set(Hnsw::<AHashSet, ArraySet<5>>::new(ibk, p)?),
			},
			5..=8 => match p.m0 {
				1..=16 => Self::H9_17(Hnsw::<ArraySet<17>, ArraySet<9>>::new(ibk, p)?),
				17..=24 => Self::H9_25(Hnsw::<ArraySet<25>, ArraySet<9>>::new(ibk, p)?),
				_ => Self::H9set(Hnsw::<AHashSet, ArraySet<9>>::new(ibk, p)?),
			},
			9..=12 => match p.m0 {
				17..=24 => Self::H13_25(Hnsw::<ArraySet<25>, ArraySet<13>>::new(ibk, p)?),
				_ => Self::H13set(Hnsw::<AHashSet, ArraySet<13>>::new(ibk, p)?),
			},
			13..=16 => Self::H17set(Hnsw::<AHashSet, ArraySet<17>>::new(ibk, p)?),
			17..=20 => Self::H21set(Hnsw::<AHashSet, ArraySet<21>>::new(ibk, p)?),
			21..=24 => Self::H25set(Hnsw::<AHashSet, ArraySet<25>>::new(ibk, p)?),
			25..=28 => Self::H29set(Hnsw::<AHashSet, ArraySet<29>>::new(ibk, p)?),
			_ => Self::Hset(Hnsw::<AHashSet, AHashSet>::new(ibk, p)?),
		};
		Ok(res)
	}
```
**File:** crates/core/src/idx/trees/hnsw/flavor.rs (L76-93)
```rust
	pub(super) async fn insert(&mut self, tx: &Transaction, q_pt: Vector) -> Result<ElementId> {
		match self {
			HnswFlavor::H5_9(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H5_17(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H5_25(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H5set(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H9_17(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H9_25(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H9set(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H13_25(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H13set(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H17set(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H21set(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H25set(h) => h.insert(tx, q_pt).await,
			HnswFlavor::H29set(h) => h.insert(tx, q_pt).await,
			HnswFlavor::Hset(h) => h.insert(tx, q_pt).await,
		}
	}
```
**File:** crates/core/src/kvs/index.rs (L147-184)
```rust
#[derive(Clone)]
pub(crate) struct IndexBuilder {
	tf: TransactionFactory,
	indexes: Arc<DashMap<IndexKey, IndexBuilding>>,
}

impl IndexBuilder {
	pub(super) fn new(tf: TransactionFactory) -> Self {
		Self {
			tf,
			indexes: Default::default(),
		}
	}

	fn start_building(
		&self,
		ctx: &Context,
		opt: Options,
		ns: NamespaceId,
		db: DatabaseId,
		ix: Arc<IndexDefinition>,
		sdr: Option<Sender<Result<()>>>,
	) -> Result<IndexBuilding> {
		let building = Arc::new(Building::new(ctx, self.tf.clone(), opt, ns, db, ix)?);
		let b = building.clone();
		let jh = task::spawn(async move {
			let r = b.run().await;
			if let Err(err) = &r {
				b.set_status(BuildingStatus::Error(err.to_string())).await;
			}
			if let Some(s) = sdr {
				if s.send(r).is_err() {
					warn!("Failed to send index building result to the consumer");
				}
			}
		});
		Ok((building, jh))
	}
```
**File:** crates/core/src/doc/index.rs (L100-112)
```rust
		#[cfg(not(target_family = "wasm"))]
		let (o, n) = if let Some(ib) = ctx.get_index_builder() {
			match ib.consume(db, ctx, ix, o, n, rid).await? {
				// The index builder consumed the value, which means it is currently building the
				// index asynchronously, we don't index the document and let the index builder
				// do it later.
				ConsumeResult::Enqueued => return Ok(()),
				// The index builder is done, the index has been built; we can proceed normally
				ConsumeResult::Ignored(o, n) => (o, n),
			}
		} else {
			(o, n)
		};
```
**File:** crates/core/src/idx/trees/hnsw/mod.rs (L137-167)
```rust
	async fn insert_level(
		&mut self,
		tx: &Transaction,
		q_pt: Vector,
		q_level: usize,
	) -> Result<ElementId> {
		// Attributes an ID to the vector
		let q_id = self.elements.next_element_id();
		let top_up_layers = self.layers.len();

		// Be sure we have existing (up) layers if required
		for i in top_up_layers..q_level {
			self.layers.push(HnswLayer::new(self.ikb.clone(), i + 1, self.m));
			self.state.layers.push(LayerState::default());
		}

		// Store the vector
		let pt_ser = SerializedVector::from(&q_pt);
		let q_pt = self.elements.insert(tx, q_id, q_pt, &pt_ser).await?;

		if let Some(ep_id) = self.state.enter_point {
			// We already have an enter_point, let's insert the element in the layers
			self.insert_element(tx, q_id, &q_pt, q_level, ep_id, top_up_layers).await?;
		} else {
			// Otherwise is the first element
			self.insert_first_element(tx, q_id, q_level).await?;
		}

		self.state.next_element_id = self.elements.inc_next_element_id();
		Ok(q_id)
	}
```
## Q3
è¿™æ · ä½ å’Œæˆ‘è¯´ä½¿ç”¨Surrealdbçš„nodejs sdkå¦‚ä½•è°ƒç”¨apiæ¥å®ç°ï¼Ÿ
### Answer
åŸºäº SurrealDB çš„å‘é‡æœç´¢åŠŸèƒ½ï¼Œæˆ‘æ¥ä¸ºä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Node.js SDK æ¥å®ç°è¿™äº›æ“ä½œã€‚

## Node.js SDK å‘é‡æœç´¢å®ç°

### 1. è¿æ¥æ•°æ®åº“å’ŒåŸºæœ¬è®¾ç½®

```javascript
import { Surreal } from 'surrealdb.js';

// è¿æ¥åˆ°æ•°æ®åº“
const db = new Surreal();
await db.connect('http://localhost:8000/rpc');
await db.signin({
  username: 'root',
  password: 'root'
});
await db.use({ namespace: 'test', database: 'test' });
```

### 2. å®šä¹‰å‘é‡ç´¢å¼•

```javascript
// å®šä¹‰ HNSW ç´¢å¼• - 3ç»´å‘é‡ï¼Œä½™å¼¦è·ç¦»
await db.query(`
  DEFINE INDEX idx_embeddings_3d ON embeddings_3d 
  FIELDS vector HNSW DIMENSION 3 DIST COSINE 
  M 12 M0 24 EFC 200;
`);

// å®šä¹‰ HNSW ç´¢å¼• - 128ç»´å‘é‡ï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»
await db.query(`
  DEFINE INDEX idx_embeddings_128d ON embeddings_128d 
  FIELDS vector HNSW DIMENSION 128 DIST EUCLIDEAN 
  M 16 M0 32 EFC 200;
`);

// å®šä¹‰ M-Tree ç´¢å¼• - 768ç»´å‘é‡ï¼Œæ›¼å“ˆé¡¿è·ç¦»
await db.query(`
  DEFINE INDEX idx_embeddings_768d ON embeddings_768d 
  FIELDS vector MTREE DIMENSION 768 DIST MANHATTAN;
`);
```

### 3. æ’å…¥å‘é‡æ•°æ®

```javascript
// æ’å…¥3ç»´å‘é‡æ•°æ®
await db.create('embeddings_3d', {
  id: 'item1',
  vector: [0.1, 0.2, 0.3],
  content: 'è¿™æ˜¯3ç»´å‘é‡çš„å†…å®¹'
});

// æ’å…¥128ç»´å‘é‡æ•°æ®
const vector128d = new Array(128).fill(0).map(() => Math.random());
await db.create('embeddings_128d', {
  id: 'item2', 
  vector: vector128d,
  content: 'è¿™æ˜¯128ç»´å‘é‡çš„å†…å®¹'
});

// æ’å…¥768ç»´å‘é‡æ•°æ®
const vector768d = new Array(768).fill(0).map(() => Math.random());
await db.create('embeddings_768d', {
  id: 'item3',
  vector: vector768d,
  content: 'è¿™æ˜¯768ç»´å‘é‡çš„å†…å®¹'
});
```

### 4. å•ç‹¬å‘é‡æœç´¢

```javascript
// åœ¨3ç»´å‘é‡è¡¨ä¸­æœç´¢
const query3d = [0.15, 0.25, 0.35];
const results3d = await db.query(`
  LET $vec = $query3d;
  SELECT id, content, vector::distance::knn() as distance 
  FROM embeddings_3d 
  WHERE vector <|5,100|> $vec
  ORDER BY distance
  LIMIT 5;
`, { query3d });

// åœ¨128ç»´å‘é‡è¡¨ä¸­æœç´¢
const query128d = new Array(128).fill(0).map(() => Math.random());
const results128d = await db.query(`
  LET $vec = $query128d;
  SELECT id, content, vector::distance::knn() as distance 
  FROM embeddings_128d 
  WHERE vector <|10,150|> $vec
  ORDER BY distance
  LIMIT 10;
`, { query128d });

// åœ¨768ç»´å‘é‡è¡¨ä¸­æœç´¢ï¼ˆä½¿ç”¨M-Treeç²¾ç¡®æœç´¢ï¼‰
const query768d = new Array(768).fill(0).map(() => Math.random());
const results768d = await db.query(`
  LET $vec = $query768d;
  SELECT id, content, vector::distance::knn() as distance 
  FROM embeddings_768d 
  WHERE vector <|8|> $vec
  ORDER BY distance
  LIMIT 8;
`, { query768d });
```

### 5. æ··åˆæœç´¢ï¼ˆè·¨è¡¨èåˆï¼‰

#### RRF èåˆæœç´¢

```javascript
const mixedQuery = [0.1, 0.2, 0.3]; // 3ç»´æŸ¥è¯¢å‘é‡
const mixedQuery128d = new Array(128).fill(0).map(() => Math.random()); // 128ç»´æŸ¥è¯¢å‘é‡

const rrfResults = await db.query(`
  -- ä»3ç»´å‘é‡è¡¨æœç´¢
  LET $results_3d = SELECT id, content, vector::distance::knn() as distance 
                    FROM embeddings_3d 
                    WHERE vector <|5|> $mixedQuery;
                    
  -- ä»128ç»´å‘é‡è¡¨æœç´¢  
  LET $results_128d = SELECT id, content, vector::distance::knn() as distance 
                      FROM embeddings_128d 
                      WHERE vector <|5|> $mixedQuery128d;
  
  -- ä½¿ç”¨RRFèåˆç»“æœ
  RETURN search::rrf([$results_3d, $results_128d], 10, 60);
`, { 
  mixedQuery, 
  mixedQuery128d 
});
```

#### Linear åŠ æƒèåˆæœç´¢

```javascript
const linearResults = await db.query(`
  -- ä»ä¸åŒè¡¨æœç´¢
  LET $results_3d = SELECT id, content, vector::distance::knn() as distance 
                    FROM embeddings_3d 
                    WHERE vector <|5|> $mixedQuery;
                    
  LET $results_128d = SELECT id, content, vector::distance::knn() as distance 
                      FROM embeddings_128d 
                      WHERE vector <|5|> $mixedQuery128d;
  
  -- ä½¿ç”¨åŠ æƒçº¿æ€§ç»„åˆèåˆï¼ˆ3ç»´è¡¨æƒé‡2.0ï¼Œ128ç»´è¡¨æƒé‡1.0ï¼‰
  RETURN search::linear([$results_3d, $results_128d], [2.0, 1.0], 10, 'minmax');
`, { 
  mixedQuery, 
  mixedQuery128d 
});
```

### 6. å¼‚æ­¥ç´¢å¼•æ„å»º

```javascript
// å¼‚æ­¥æ„å»ºå¤§å‹ç´¢å¼•
await db.query(`
  -- ä½¿ç”¨ CONCURRENTLY å…³é”®å­—å¼‚æ­¥æ„å»ºç´¢å¼•
  DEFINE INDEX CONCURRENTLY idx_large_vector ON large_vectors 
  FIELDS embedding HNSW DIMENSION 1536 DIST COSINE 
  M 16 M0 32 EFC 200;
`);

// æ£€æŸ¥ç´¢å¼•æ„å»ºçŠ¶æ€
const indexStatus = await db.query(`
  SELECT * FROM information() WHERE name = 'idx_large_vector';
`);
```

### 7. å®Œæ•´çš„æœç´¢å‡½æ•°å°è£…

```javascript
class VectorSearchService {
  constructor(db) {
    this.db = db;
  }

  // å•è¡¨å‘é‡æœç´¢
  async searchSingleTable(tableName, queryVector, k = 5, ef = 100) {
    const result = await this.db.query(`
      LET $vec = $queryVector;
      SELECT id, content, vector::distance::knn() as distance 
      FROM type::table($tableName) 
      WHERE vector <|$k,$ef|> $vec
      ORDER BY distance
      LIMIT $k;
    `, { tableName, queryVector, k, ef });
    
    return result[0]?.result || [];
  }

  // æ··åˆæœç´¢ - RRF
  async searchMixedRRF(searchConfigs, k = 10, rrfConstant = 60) {
    const searchQueries = searchConfigs.map((config, index) => `
      LET $results_${index} = SELECT id, content, vector::distance::knn() as distance 
                             FROM type::table($tableName_${index}) 
                             WHERE vector <|$k_${index}|> $queryVector_${index};
    `).join('\n');

    const rrfQuery = `
      ${searchQueries}
      RETURN search::rrf([${searchConfigs.map((_, i) => `$results_${i}`).join(', ')}], $k, $rrfConstant);
    `;

    const params = {
      k,
      rrfConstant,
      ...searchConfigs.reduce((acc, config, index) => {
        acc[`tableName_${index}`] = config.tableName;
        acc[`queryVector_${index}`] = config.queryVector;
        acc[`k_${index}`] = config.k || 5;
        return acc;
      }, {})
    };

    const result = await this.db.query(rrfQuery, params);
    return result[0]?.result || [];
  }

  // æ··åˆæœç´¢ - Linear
  async searchMixedLinear(searchConfigs, weights, k = 10, normalization = 'minmax') {
    const searchQueries = searchConfigs.map((config, index) => `
      LET $results_${index} = SELECT id, content, vector::distance::knn() as distance 
                             FROM type::table($tableName_${index}) 
                             WHERE vector <|$k_${index}|> $queryVector_${index};
    `).join('\n');

    const linearQuery = `
      ${searchQueries}
      RETURN search::linear([${searchConfigs.map((_, i) => `$results_${i}`).join(', ')}], 
                           $weights, $k, $normalization);
    `;

    const params = {
      k,
      weights,
      normalization,
      ...searchConfigs.reduce((acc, config, index) => {
        acc[`tableName_${index}`] = config.tableName;
        acc[`queryVector_${index}`] = config.queryVector;
        acc[`k_${index}`] = config.k || 5;
        return acc;
      }, {})
    };

    const result = await this.db.query(linearQuery, params);
    return result[0]?.result || [];
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const searchService = new VectorSearchService(db);

// å•è¡¨æœç´¢
const singleResults = await searchService.searchSingleTable(
  'embeddings_3d', 
  [0.1, 0.2, 0.3], 
  5, 
  100
);

// æ··åˆæœç´¢
const mixedResults = await searchService.searchMixedRRF([
  { tableName: 'embeddings_3d', queryVector: [0.1, 0.2, 0.3], k: 5 },
  { tableName: 'embeddings_128d', queryVector: new Array(128).fill(0).map(() => Math.random()), k: 8 }
], 10, 60);
```

## Notes

**å…³é”®è¦ç‚¹ï¼š**

1. **è¿æ¥ç®¡ç†**ï¼šä½¿ç”¨ `surrealdb.js` SDK è¿æ¥åˆ° SurrealDB æœåŠ¡å™¨
2. **ç´¢å¼•å®šä¹‰**ï¼šé€šè¿‡ SurrealQL çš„ `DEFINE INDEX` è¯­å¥åˆ›å»ºå‘é‡ç´¢å¼•
3. **å‘é‡æœç´¢**ï¼šä½¿ç”¨ `<|k,ef|>` æ“ä½œç¬¦è¿›è¡Œ HNSW æœç´¢ï¼Œ`<|k|>` è¿›è¡Œ M-Tree æœç´¢
4. **æ··åˆæœç´¢**ï¼šåˆ©ç”¨ `search::rrf()` å’Œ `search::linear()` å‡½æ•°å®ç°è·¨è¡¨ç»“æœèåˆ
5. **å¼‚æ­¥æ„å»º**ï¼šä½¿ç”¨ `CONCURRENTLY` å…³é”®å­—å®ç°å¤§å‹ç´¢å¼•çš„å¼‚æ­¥æ„å»º
6. **å‚æ•°åŒ–æŸ¥è¯¢**ï¼šä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢ SQL æ³¨å…¥å¹¶æé«˜æ€§èƒ½

è¿™äº›ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ Node.js ç¯å¢ƒä¸­å……åˆ†åˆ©ç”¨ SurrealDB çš„å‘é‡æœç´¢åŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤„ç†ä¸åŒç»´åº¦çš„å‘é‡è¡¨å’Œå®ç°å¤æ‚çš„æ··åˆæœç´¢ç­–ç•¥ã€‚

Wiki pages you might want to explore:
- [Core Database Engine (surrealdb/surrealdb)](/wiki/surrealdb/surrealdb#6)
## Q4
æˆ‘æ˜¯è¯´å®ç°å…ˆæ’å…¥å‘é‡åˆ°databaseçš„å¯¹åº”ç»´åº¦çš„å‘é‡è¡¨ ç„¶åå†å¢é‡æ„å»ºhnsw
å› ä¸ºå†™å…¥æ•°æ®æ˜¯ä¸€ä¸ªå¿«è¿‡ç¨‹ è€Œæ„å»ºhnswæ˜¯ä¸€ä¸ªæ…¢çš„è¿‡ç¨‹
### Answer
åŸºäºä½ çš„éœ€æ±‚ï¼Œæˆ‘æ¥å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Node.js SDK å®ç°å…ˆæ’å…¥å‘é‡æ•°æ®ï¼Œç„¶åå¼‚æ­¥å¢é‡æ„å»º HNSW ç´¢å¼•çš„æ–¹æ¡ˆã€‚

## å®ç°æ–¹æ¡ˆï¼šåˆ†ç¦»å‘é‡æ’å…¥ä¸HNSWæ„å»º

### 1. å…ˆåˆ›å»ºè¡¨å’Œæ’å…¥å‘é‡æ•°æ®ï¼ˆä¸æ„å»ºç´¢å¼•ï¼‰

```javascript
import { Surreal } from 'surrealdb.js';

const db = new Surreal();
await db.connect('http://localhost:8000/rpc');
await db.signin({ username: 'root', password: 'root' });
await db.use({ namespace: 'test', database: 'test' });

// åˆ›å»ºå‘é‡è¡¨ï¼ˆå…ˆä¸åˆ›å»ºç´¢å¼•ï¼‰
await db.query(`
  CREATE TABLE embeddings_3d;
  CREATE TABLE embeddings_128d;
  CREATE TABLE embeddings_768d;
`);

// æ‰¹é‡æ’å…¥å‘é‡æ•°æ®ï¼ˆå¿«é€Ÿå†™å…¥ï¼‰
async function batchInsertVectors(tableName, vectors) {
  const batchSize = 1000;
  for (let i = 0; i < vectors.length; i += batchSize) {
    const batch = vectors.slice(i, i + batchSize);
    const promises = batch.map((vector, index) => 
      db.create(tableName, {
        id: `item_${i + index}`,
        vector: vector,
        content: `Content for item ${i + index}`,
        timestamp: Date.now()
      })
    );
    await Promise.all(promises);
    console.log(`Inserted ${Math.min(i + batchSize, vectors.length)} vectors into ${tableName}`);
  }
}

// æ’å…¥ä¸åŒç»´åº¦çš„å‘é‡æ•°æ®
const vectors3d = Array.from({ length: 10000 }, () => 
  [Math.random(), Math.random(), Math.random()]
);

const vectors128d = Array.from({ length: 5000 }, () => 
  Array.from({ length: 128 }, () => Math.random())
);

const vectors768d = Array.from({ length: 2000 }, () => 
  Array.from({ length: 768 }, () => Math.random())
);

// å¹¶è¡Œæ’å…¥æ‰€æœ‰å‘é‡æ•°æ®
console.time('Vector insertion');
await Promise.all([
  batchInsertVectors('embeddings_3d', vectors3d),
  batchInsertVectors('embeddings_128d', vectors128d),
  batchInsertVectors('embeddings_768d', vectors768d)
]);
console.timeEnd('Vector insertion');
```

### 2. å¼‚æ­¥æ„å»ºHNSWç´¢å¼•

```javascript
// ä½¿ç”¨ CONCURRENTLY å…³é”®å­—å¼‚æ­¥æ„å»ºç´¢å¼•
async function buildIndexesConcurrently() {
  console.log('Starting concurrent index building...');
  
  const indexDefinitions = [
    {
      name: 'idx_embeddings_3d',
      table: 'embeddings_3d',
      dimension: 3,
      distance: 'COSINE',
      params: 'M 12 M0 24 EFC 200'
    },
    {
      name: 'idx_embeddings_128d', 
      table: 'embeddings_128d',
      dimension: 128,
      distance: 'EUCLIDEAN',
      params: 'M 16 M0 32 EFC 200'
    },
    {
      name: 'idx_embeddings_768d',
      table: 'embeddings_768d', 
      dimension: 768,
      distance: 'MANHATTAN',
      params: 'M 20 M0 40 EFC 200'
    }
  ];

  // å¹¶è¡Œå¯åŠ¨æ‰€æœ‰ç´¢å¼•æ„å»º
  const buildPromises = indexDefinitions.map(async (idx) => {
    console.log(`Starting index build for ${idx.name}`);
    await db.query(`
      DEFINE INDEX CONCURRENTLY ${idx.name} ON ${idx.table} 
      FIELDS vector HNSW DIMENSION ${idx.dimension} DIST ${idx.distance} 
      ${idx.params};
    `);
    console.log(`Index build initiated for ${idx.name}`);
  });

  await Promise.all(buildPromises);
  console.log('All index builds initiated');
}

// å¯åŠ¨å¼‚æ­¥ç´¢å¼•æ„å»º
console.time('Index building initiation');
await buildIndexesConcurrently();
console.timeEnd('Index building initiation');
```

### 3. ç›‘æ§ç´¢å¼•æ„å»ºçŠ¶æ€

```javascript
// æ£€æŸ¥ç´¢å¼•æ„å»ºçŠ¶æ€
async function monitorIndexBuilding() {
  const indexNames = ['idx_embeddings_3d', 'idx_embeddings_128d', 'idx_embeddings_768d'];
  
  const checkStatus = async () => {
    const statuses = await Promise.all(
      indexNames.map(async (name) => {
        try {
          const result = await db.query(`
            SELECT * FROM information() WHERE name = '${name}';
          `);
          return { name, status: result[0]?.result?.[0] || 'Not found' };
        } catch (error) {
          return { name, status: 'Error', error: error.message };
        }
      })
    );
    
    console.log('Index building status:');
    statuses.forEach(({ name, status, error }) => {
      console.log(`  ${name}: ${status}${error ? ` (${error})` : ''}`);
    });
    
    // æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç´¢å¼•éƒ½æ„å»ºå®Œæˆ
    const allCompleted = statuses.every(s => 
      s.status === 'Ready' || s.status.includes('ready')
    );
    
    if (!allCompleted) {
      setTimeout(checkStatus, 5000); // 5ç§’åå†æ¬¡æ£€æŸ¥
    } else {
      console.log('All indexes are ready!');
    }
  };
  
  checkStatus();
}

// å¼€å§‹ç›‘æ§
await monitorIndexBuilding();
```

### 4. å¢é‡æ•°æ®æ’å…¥å’Œç´¢å¼•æ›´æ–°

```javascript
// åœ¨ç´¢å¼•æ„å»ºè¿‡ç¨‹ä¸­æˆ–å®Œæˆåï¼Œç»§ç»­æ’å…¥æ–°æ•°æ®
async function incrementalInsert(tableName, newVectors) {
  console.log(`Inserting ${newVectors.length} new vectors into ${tableName}`);
  
  for (let i = 0; i < newVectors.length; i++) {
    await db.create(tableName, {
      id: `new_item_${Date.now()}_${i}`,
      vector: newVectors[i],
      content: `New content ${i}`,
      timestamp: Date.now()
    });
  }
  
  console.log(`Incremental insert completed for ${tableName}`);
}

// ç¤ºä¾‹ï¼šåœ¨ç´¢å¼•æ„å»ºæœŸé—´æ’å…¥æ–°æ•°æ®
setTimeout(async () => {
  const newVectors3d = Array.from({ length: 100 }, () => 
    [Math.random(), Math.random(), Math.random()]
  );
  await incrementalInsert('embeddings_3d', newVectors3d);
}, 10000); // 10ç§’åæ’å…¥æ–°æ•°æ®
```

### 5. å®Œæ•´çš„å·¥ä½œæµç¨‹ç®¡ç†

```javascript
class VectorIndexManager {
  constructor(db) {
    this.db = db;
    this.indexStatus = new Map();
  }

  // ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿæ’å…¥å‘é‡æ•°æ®
  async insertVectors(tableName, vectors, batchSize = 1000) {
    console.log(`Starting fast insertion for ${tableName}...`);
    const startTime = Date.now();
    
    for (let i = 0; i < vectors.length; i += batchSize) {
      const batch = vectors.slice(i, i + batchSize);
      const promises = batch.map((vector, index) => 
        this.db.create(tableName, {
          id: `${tableName}_${i + index}`,
          vector: vector,
          content: `Content ${i + index}`,
          batch_id: Math.floor(i / batchSize)
        })
      );
      await Promise.all(promises);
    }
    
    const duration = Date.now() - startTime;
    console.log(`Inserted ${vectors.length} vectors into ${tableName} in ${duration}ms`);
    return duration;
  }

  // ç¬¬äºŒé˜¶æ®µï¼šå¼‚æ­¥æ„å»ºç´¢å¼•
  async buildIndexAsync(indexConfig) {
    const { name, table, dimension, distance, params } = indexConfig;
    console.log(`Starting async index build for ${name}...`);
    
    try {
      await this.db.query(`
        DEFINE INDEX CONCURRENTLY ${name} ON ${table} 
        FIELDS vector HNSW DIMENSION ${dimension} DIST ${distance} 
        ${params};
      `);
      
      this.indexStatus.set(name, 'building');
      console.log(`Index build initiated: ${name}`);
      return true;
    } catch (error) {
      console.error(`Failed to start index build for ${name}:`, error);
      this.indexStatus.set(name, 'error');
      return false;
    }
  }

  // ç›‘æ§æ„å»ºçŠ¶æ€
  async monitorIndexes(indexNames) {
    const checkInterval = 5000; // 5ç§’æ£€æŸ¥ä¸€æ¬¡
    
    const checkStatus = async () => {
      const results = await Promise.all(
        indexNames.map(async (name) => {
          try {
            const result = await this.db.query(`
              SELECT status FROM information() WHERE name = '${name}';
            `);
            const status = result[0]?.result?.[0]?.status || 'unknown';
            this.indexStatus.set(name, status);
            return { name, status };
          } catch (error) {
            return { name, status: 'error', error: error.message };
          }
        })
      );

      console.log('Index Status Update:');
      results.forEach(({ name, status }) => {
        console.log(`  ${name}: ${status}`);
      });

      const allReady = results.every(r => r.status === 'ready');
      if (!allReady) {
        setTimeout(checkStatus, checkInterval);
      } else {
        console.log('ğŸ‰ All indexes are ready for searching!');
      }
    };

    checkStatus();
  }

  // å®Œæ•´å·¥ä½œæµç¨‹
  async runWorkflow() {
    console.log('ğŸš€ Starting vector insertion and index building workflow...');
    
    // é˜¶æ®µ1ï¼šå¿«é€Ÿæ’å…¥æ‰€æœ‰å‘é‡æ•°æ®
    const insertPromises = [
      this.insertVectors('embeddings_3d', vectors3d),
      this.insertVectors('embeddings_128d', vectors128d),
      this.insertVectors('embeddings_768d', vectors768d)
    ];
    
    console.time('Total insertion time');
    await Promise.all(insertPromises);
    console.timeEnd('Total insertion time');
    
    // é˜¶æ®µ2ï¼šå¼‚æ­¥æ„å»ºæ‰€æœ‰ç´¢å¼•
    const indexConfigs = [
      { name: 'idx_embeddings_3d', table: 'embeddings_3d', dimension: 3, distance: 'COSINE', params: 'M 12 M0 24 EFC 200' },
      { name: 'idx_embeddings_128d', table: 'embeddings_128d', dimension: 128, distance: 'EUCLIDEAN', params: 'M 16 M0 32 EFC 200' },
      { name: 'idx_embeddings_768d', table: 'embeddings_768d', dimension: 768, distance: 'MANHATTAN', params: 'M 20 M0 40 EFC 200' }
    ];
    
    console.time('Index building initiation');
    const buildPromises = indexConfigs.map(config => this.buildIndexAsync(config));
    await Promise.all(buildPromises);
    console.timeEnd('Index building initiation');
    
    // é˜¶æ®µ3ï¼šç›‘æ§æ„å»ºçŠ¶æ€
    await this.monitorIndexes(indexConfigs.map(c => c.name));
  }
}

// ä½¿ç”¨å·¥ä½œæµç¨‹ç®¡ç†å™¨
const manager = new VectorIndexManager(db);
await manager.runWorkflow();
```

## å…³é”®ä¼˜åŠ¿

è¿™ä¸ªæ–¹æ¡ˆå®ç°äº†ä½ è¦æ±‚çš„åˆ†ç¦»ï¼š

1. **å¿«é€Ÿæ•°æ®æ’å…¥**ï¼šå‘é‡æ•°æ®ç›´æ¥å­˜å‚¨åˆ°è¡¨ä¸­ï¼Œä¸ç­‰å¾…HNSWæ„å»º surrealdb:223-238 
2. **å¼‚æ­¥ç´¢å¼•æ„å»º**ï¼šä½¿ç”¨`CONCURRENTLY`å…³é”®å­—è®©IndexBuilderåœ¨åå°æ„å»ºHNSW surrealdb:166-168 
3. **å¢é‡æ›´æ–°æ”¯æŒ**ï¼šæ–°æ’å…¥çš„æ•°æ®ä¼šè¢«IndexBuilderè‡ªåŠ¨å¤„ç† surrealdb:101-112 
4. **çŠ¶æ€ç›‘æ§**ï¼šå¯ä»¥å®æ—¶ç›‘æ§ç´¢å¼•æ„å»ºè¿›åº¦

è¿™æ ·ä½ å¯ä»¥å…ˆå¿«é€Ÿå®Œæˆæ•°æ®å†™å…¥ï¼Œç„¶åè®©ç³»ç»Ÿåœ¨åå°æ…¢æ…¢æ„å»ºHNSWç´¢å¼•ï¼Œä¸ä¼šé˜»å¡åç»­çš„æ•°æ®æ“ä½œã€‚

## Notes

**é‡è¦æé†’ï¼š**

1. **CONCURRENTLYå…³é”®å­—**ï¼šè¿™æ˜¯å®ç°å¼‚æ­¥æ„å»ºçš„å…³é”®ï¼Œå®ƒä¼šè®©IndexBuilderåœ¨åå°å¤„ç†ç´¢å¼•æ„å»º surrealdb:1168-1171 
2. **æ•°æ®ä¸€è‡´æ€§**ï¼šåœ¨ç´¢å¼•æ„å»ºæœŸé—´ï¼Œå‘é‡æœç´¢å¯èƒ½è¿”å›ä¸å®Œæ•´çš„ç»“æœï¼Œå»ºè®®ç­‰ç´¢å¼•æ„å»ºå®Œæˆåå†è¿›è¡Œç”Ÿäº§æœç´¢
3. **èµ„æºæ¶ˆè€—**ï¼šå¼‚æ­¥æ„å»ºä¼šæ¶ˆè€—é¢å¤–çš„CPUå’Œå†…å­˜èµ„æºï¼Œå»ºè®®åœ¨ä½å³°æœŸè¿›è¡Œå¤§è§„æ¨¡ç´¢å¼•æ„å»º
4. **é”™è¯¯å¤„ç†**ï¼šéœ€è¦ç›‘æ§ç´¢å¼•æ„å»ºçŠ¶æ€ï¼Œå¤„ç†å¯èƒ½çš„æ„å»ºå¤±è´¥æƒ…å†µ

è¿™ä¸ªæ–¹æ¡ˆå®Œç¾è§£å†³äº†"å†™å…¥å¿«ï¼Œæ„å»ºæ…¢"çš„é—®é¢˜ï¼Œè®©ä½ å¯ä»¥å¿«é€Ÿæ’å…¥å¤§é‡å‘é‡æ•°æ®ï¼Œè€Œä¸éœ€è¦ç­‰å¾…HNSWç´¢å¼•æ„å»ºå®Œæˆã€‚

Wiki pages you might want to explore:
- [Core Database Engine (surrealdb/surrealdb)](/wiki/surrealdb/surrealdb#6)
### Citations
**File:** crates/core/src/kvs/index.rs (L223-238)
```rust
	pub(crate) async fn consume(
		&self,
		db: &DatabaseDefinition,
		ctx: &Context,
		ix: &IndexDefinition,
		old_values: Option<Vec<Value>>,
		new_values: Option<Vec<Value>>,
		rid: &RecordId,
	) -> Result<ConsumeResult> {
		let key = IndexKey::new(db.namespace_id, db.database_id, &ix.what, &ix.name);
		if let Some(r) = self.indexes.get(&key) {
			let (b, _) = r.value();
			return b.maybe_consume(ctx, old_values, new_values, rid).await;
		}
		Ok(ConsumeResult::Ignored(old_values, new_values))
	}
```
**File:** crates/core/src/expr/statements/define/index.rs (L166-168)
```rust
		if self.concurrently {
			write!(f, " CONCURRENTLY")?
		}
```
**File:** crates/core/src/doc/index.rs (L101-112)
```rust
		let (o, n) = if let Some(ib) = ctx.get_index_builder() {
			match ib.consume(db, ctx, ix, o, n, rid).await? {
				// The index builder consumed the value, which means it is currently building the
				// index asynchronously, we don't index the document and let the index builder
				// do it later.
				ConsumeResult::Enqueued => return Ok(()),
				// The index builder is done, the index has been built; we can proceed normally
				ConsumeResult::Ignored(o, n) => (o, n),
			}
		} else {
			(o, n)
		};
```
**File:** crates/core/src/syn/parser/stmt/define.rs (L1168-1171)
```rust
				t!("CONCURRENTLY") => {
					self.pop_peek();
					res.concurrently = true;
				}
```
## Q5
æˆ‘æ˜¯è¯´å®ç°å…ˆæ’å…¥å‘é‡åˆ°databaseçš„å¯¹åº”ç»´åº¦çš„å‘é‡è¡¨ ç„¶åå†å¢é‡æ„å»ºhnsw
å› ä¸ºå†™å…¥æ•°æ®æ˜¯ä¸€ä¸ªå¿«è¿‡ç¨‹ è€Œæ„å»ºhnswæ˜¯ä¸€ä¸ªæ…¢çš„è¿‡ç¨‹
## SurrealDB å‘é‡æ’å…¥ä¸HNSWå¼‚æ­¥æ„å»ºåˆ†ç¦»æœºåˆ¶
è¿™ä¸ª codemap è¯¦ç»†å±•ç¤ºäº† SurrealDB å¦‚ä½•é€šè¿‡ CONCURRENTLY å…³é”®å­—å’Œ IndexBuilder ç»„ä»¶å®ç°å‘é‡æ•°æ®æ’å…¥ä¸ HNSW ç´¢å¼•æ„å»ºçš„åˆ†ç¦»ã€‚å…³é”®æµç¨‹åŒ…æ‹¬ï¼šå¼‚æ­¥ç´¢å¼•æ„å»ºå¯åŠ¨ [1c]ã€IndexBuilder ä»»åŠ¡ç®¡ç† [2c]ã€æ–‡æ¡£æ¶ˆè´¹æœºåˆ¶ [3a]ã€å¢é‡é˜Ÿåˆ—ç®¡ç† [4c]ã€ä¸¤é˜¶æ®µæ„å»ºæµç¨‹ [5b]ï¼Œä»¥åŠ HNSW å‘é‡æ’å…¥ [6b]ã€‚è¿™ç§è®¾è®¡å®ç°äº†å¿«é€Ÿæ•°æ®å†™å…¥å’Œåå°æ…¢é€Ÿç´¢å¼•æ„å»ºçš„è§£è€¦ã€‚
### 1. ä½¿ç”¨ CONCURRENTLY å…³é”®å­—å¯åŠ¨å¼‚æ­¥ç´¢å¼•æ„å»º
å±•ç¤ºå¦‚ä½•é€šè¿‡ SQL è¯­æ³•è§¦å‘å¼‚æ­¥ç´¢å¼•æ„å»ºï¼Œå®ç°æ•°æ®æ’å…¥ä¸ç´¢å¼•æ„å»ºçš„åˆ†ç¦»
### 1a. CONCURRENTLY å…³é”®å­—å¤„ç† (`index.rs:166`)
åœ¨ SQL è§£æä¸­è¯†åˆ« CONCURRENTLY å…³é”®å­—
```text
if self.concurrently {
			write!(f, " CONCURRENTLY")?
		}
```
### 1b. å¯åŠ¨ç´¢å¼•æ„å»ºæµç¨‹ (`index.rs:138`)
æ ¹æ® concurrently å‚æ•°å†³å®šæ˜¯å¦å¼‚æ­¥æ„å»ºç´¢å¼•
```text
run_indexing(stk, ctx, opt, doc, &index_def, !self.concurrently).await?;
```
### 1c. è°ƒç”¨ IndexBuilder (`index.rs:210`)
å°†ç´¢å¼•æ„å»ºä»»åŠ¡äº¤ç»™ IndexBuilder å¤„ç†
```text
let rcv = ctx
			.get_index_builder()
			.ok_or_else(|| Error::unreachable("No Index Builder"))?
			.build(ctx, opt.clone(), ns, db, index.clone().into(), _blocking)?;
```
### 2. IndexBuilder å¼‚æ­¥æ„å»ºæœºåˆ¶
å±•ç¤º IndexBuilder å¦‚ä½•ç®¡ç†åå°ç´¢å¼•æ„å»ºä»»åŠ¡ï¼Œå¤„ç†å¢é‡æ•°æ®
### 2a. æ„å»ºæ–¹æ³•å…¥å£ (`index.rs:186`)
IndexBuilder çš„ä¸»è¦æ„å»ºæ¥å£
```text
pub(crate) fn build(
		&self,
		ctx: &Context,
		opt: Options,
		ns: NamespaceId,
		db: DatabaseId,
		ix: Arc<IndexDefinition>,
		blocking: bool,
	) -> Result<Option<Receiver<Result<()>>>> {
```
### 2b. å¯åŠ¨æ„å»ºä»»åŠ¡ (`index.rs:211`)
åˆ›å»ºåå°æ„å»ºä»»åŠ¡
```text
let ib = self.start_building(ctx, opt, ns, db, ix, sdr)?;
```
### 2c. å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œ (`index.rs:172`)
åœ¨åå°ä»»åŠ¡ä¸­æ‰§è¡Œç´¢å¼•æ„å»º
```text
let jh = task::spawn(async move {
			let r = b.run().await;
			if let Err(err) = &r {
				b.set_status(BuildingStatus::Error(err.to_string())).await;
			}
			if let Some(s) = sdr {
				if s.send(r).is_err() {
					warn!("Failed to send index building result to the consumer");
				}
			}
		});
```
### 3. æ–‡æ¡£ç´¢å¼•æ—¶çš„æ¶ˆè´¹æœºåˆ¶
å±•ç¤ºæ–‡æ¡£æ’å…¥æ—¶å¦‚ä½•è¢« IndexBuilder æ¶ˆè´¹ï¼Œå®ç°å¢é‡æ„å»º
### 3a. æ£€æŸ¥ IndexBuilder çŠ¶æ€ (`index.rs:101`)
æ–‡æ¡£ç´¢å¼•æ—¶æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒçš„ç´¢å¼•æ„å»ºè¿‡ç¨‹
```text
let (o, n) = if let Some(ib) = ctx.get_index_builder() {
			match ib.consume(db, ctx, ix, o, n, rid).await? {
				// The index builder consumed the value, which means it is currently building the
				// index asynchronously, we don't index the document and let the index builder
				// do it later.
				ConsumeResult::Enqueued => return Ok(()),
				// The index builder is done, the index has been built; we can proceed normally
				ConsumeResult::Ignored(o, n) => (o, n),
			}
		} else {
			(o, n)
		};
```
### 3b. æ¶ˆè´¹æ–‡æ¡£å˜æ›´ (`index.rs:223`)
IndexBuilder æ¶ˆè´¹æ–‡æ¡£å˜æ›´çš„å…¥å£æ–¹æ³•
```text
pub(crate) async fn consume(
		&self,
		db: &DatabaseDefinition,
		ctx: &Context,
		ix: &IndexDefinition,
		old_values: Option<Vec<Value>>,
		new_values: Option<Vec<Value>>,
		rid: &RecordId,
	) -> Result<ConsumeResult> {
```
### 3c. æŸ¥æ‰¾æ´»è·ƒæ„å»ºä»»åŠ¡ (`index.rs:233`)
æŸ¥æ‰¾å¯¹åº”çš„æ´»è·ƒç´¢å¼•æ„å»ºä»»åŠ¡
```text
if let Some(r) = self.indexes.get(&key) {
			let (b, _) = r.value();
			return b.maybe_consume(ctx, old_values, new_values, rid).await;
		}
```
### 4. å¢é‡æ•°æ®é˜Ÿåˆ—ç®¡ç†
å±•ç¤ºå¦‚ä½•åœ¨ç´¢å¼•æ„å»ºæœŸé—´ç®¡ç†å¢é‡æ•°æ®çš„é˜Ÿåˆ—
### 4a. æ¶ˆè´¹å†³ç­–é€»è¾‘ (`index.rs:369`)
å†³å®šæ˜¯å¦å°†æ–‡æ¡£åŠ å…¥é˜Ÿåˆ—æˆ–ç›´æ¥ç´¢å¼•
```text
async fn maybe_consume(
		&self,
		ctx: &Context,
		old_values: Option<Vec<Value>>,
		new_values: Option<Vec<Value>>,
		rid: &RecordId,
	) -> Result<ConsumeResult> {
```
### 4b. æ£€æŸ¥æ„å»ºçŠ¶æ€ (`index.rs:379`)
å¦‚æœç´¢å¼•å·²æ„å»ºå®Œæˆä¸”é˜Ÿåˆ—ä¸ºç©ºï¼Œç›´æ¥å¤„ç†æ–‡æ¡£
```text
if queue.is_empty() {
			// If the appending queue is empty and the index is built...
			if self.status.read().await.is_ready() {
				// ... we return the values back, so the document can be updated the usual way
				return Ok(ConsumeResult::Ignored(old_values, new_values));
			}
		}
```
### 4c. åŠ å…¥å¢é‡é˜Ÿåˆ— (`index.rs:388`)
å°†æ–‡æ¡£å˜æ›´å­˜å‚¨åˆ°å¢é‡é˜Ÿåˆ—ä¸­
```text
let a = Appending {
			old_values,
			new_values,
			id: rid.key.clone(),
		};
		// Get the idx of this appended record from the sequence
		let idx = queue.add_update();
		// Store the appending
		let ia = self.ikb.new_ia_key(idx);
		tx.set(&ia, &a, None).await?;
```
### 5. HNSW ç´¢å¼•çš„ä¸¤é˜¶æ®µæ„å»º
å±•ç¤º HNSW ç´¢å¼•å¦‚ä½•å…ˆå¤„ç†åˆå§‹æ•°æ®ï¼Œå†å¤„ç†å¢é‡æ›´æ–°
### 5a. å¼€å§‹æ„å»ºæµç¨‹ (`index.rs:419`)
æ¸…ç†æ—§ç´¢å¼•æ•°æ®ï¼Œå¼€å§‹æ„å»ºè¿‡ç¨‹
```text
async fn run(&self) -> Result<()> {
		// Remove the index data
		{
			self.set_status(BuildingStatus::Cleaning).await;
```
### 5b. åˆå§‹æ•°æ®ç´¢å¼• (`index.rs:432`)
ç¬¬ä¸€é˜¶æ®µï¼šç´¢å¼•è¡¨ä¸­æ‰€æœ‰ç°æœ‰æ•°æ®
```text
// First iteration, we index every key
		let beg = thing::prefix(self.ns, self.db, self.ikb.table())?;
		let end = thing::suffix(self.ns, self.db, self.ikb.table())?;
```
### 5c. å¢é‡æ•°æ®å¤„ç† (`index.rs:472`)
ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†æ„å»ºæœŸé—´çš„å¢é‡å˜æ›´
```text
// Second iteration, we index/remove any records that has been added or removed
		// since the initial indexing
```
### 6. HNSW å‘é‡æ’å…¥æœºåˆ¶
å±•ç¤º HNSW ç´¢å¼•å¦‚ä½•åŒæ—¶æ›´æ–°å‘é‡è¡¨å’Œå›¾ç»“æ„
### 6a. HNSW æ–‡æ¡£ç´¢å¼• (`index.rs:91`)
HNSW ç´¢å¼•çš„æ–‡æ¡£ç´¢å¼•å…¥å£
```text
pub async fn index_document(
		&mut self,
		tx: &Transaction,
		id: &RecordIdKey,
		content: &[Value],
	) -> Result<()> {
```
### 6b. æ’å…¥å‘é‡æ•°æ® (`index.rs:107`)
åŒæ—¶æ›´æ–°å‘é‡è¡¨å’Œ HNSW å›¾ç»“æ„
```text
self.vec_docs.insert(tx, vector, doc_id, &mut self.hnsw).await?;
```
### 6c. æ„å»º HNSW å›¾ (`docs.rs:188`)
åœ¨ HNSW å›¾ä¸­æ’å…¥æ–°å‘é‡èŠ‚ç‚¹
```text
None => {
				//  We don't have the vector, we insert it in the graph
				let element_id = h.insert(tx, o).await?;
```